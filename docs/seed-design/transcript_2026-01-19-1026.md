# Interview Transcript — Session 1 (continued)

**Started:** 2026-01-19 10:26

## Notation

- **[U]** Plain text — User's words (high fidelity)
- **[C]** Code block — My responses
- **[T]** *Italics* — My thinking/reasoning
- **[S]** after `___` — Synthesis
- **[?]** Open question — Tracked in process doc


---

## The Grand Idea — Problem Statement

[10:26]

**[U]** The idea is to build a multi-agent orchestration framework and user interface specifically tailored to multi-agent swarms. The gist of the problem statement is that:

- If I use one Claude Code instance, there's a lot of waiting around time, where I don't want to switch completely away, but Claude needs a few minutes to work unattended. This is low efficiency.

- If I use more than one, I have to keep an eye on both, but getting many requests for permissions when one is working unattended can be interruptive and hard to maintain a discussion with the second.

- If I use >5 sessions, it's too much to keep track of while also engaging with any one session.

- YOLO mode needs a sandbox, the built in one doesn't suit my needs, 3rd party tools make assumptions about git and worktrees that are not correct for me.

- IDE integration is janky in the terminal, and using the VSCode plugin means some features don't quite work right.

- There's no good tool to balance the need to jump into code or documents myself, vs when to let the agent do its own thing.

- Agents run in a free-for-all, where we're lucky if they do everything perfect, and most fixes for this turn into a complex pipeline anyway.

- Agent runtimes that are text user interfaces in a terminal are hard to manage elegantly, and everyone just settles on something tmux based.

- Too many agents clobber each other when working on the same things, tools to manage worktrees make gross assumptions about how you use code repos that aren't valid in my use case.

- Many of my workflows require a deterministic shell around them to enforce that they are performed correctly, and the Claude Code TUI just offers hooks that can constrain this, without giving enough flexibility.

- Many orchestration frameworks don't manage the "efficiency of quality" and allow agents to burn too many tokens.

- Many frameworks also have a hard time breaking up different tasks and ensuring they use the correct workflow for it, or they just maintain overly rigid workflows.

- There's no self improvement of these systems.

- There isn't enough telemetry to fully improve it, there's a heavy reliance on brute force here.

**[U]** Digest this, let me know if you think there are other likely pain points here that I have and missed. Either ask questions now, or add them to the notes to revisit later after we start talking about the idea itself. Then we can start diving into a more positive definition of this (namely what this thing is, instead of a negative definition of what the thing isn't).

**[U]** Oh, btw, feel free to fix my spelling mistakes on the verbatim transcripts.

**[T]** *Digesting the problem statement. I see 14 distinct pain points spanning: efficiency (single vs multi-agent tradeoffs), attention management (tracking multiple sessions), sandboxing (existing tools don't fit), UI/UX (terminal TUI, IDE integration), coordination (agents clobbering each other, worktree assumptions), workflow rigidity vs flexibility, token efficiency, self-improvement, and telemetry gaps. Let me think about what might be missing from this list...*

**[C]**
```
Potential additional pain points you might have (to confirm or deny):

1. Context handoff — When one agent discovers something another needs, how does
   that knowledge transfer? Currently feels like each agent is isolated.

2. Debugging failed runs — When an agent goes off the rails, understanding the
   chain of decisions that led there. Postmortems are hard.

3. Cost visibility — Tracking token spend across agents, budgeting, knowing which
   tasks are expensive vs cheap.

4. Priority/attention routing — Which agent deserves your attention right now?
   Currently you're the scheduler.

5. State persistence — What happens when a session dies mid-task? Resumption is
   often "start over with context summary."

6. Merge/integration pain — Beyond clobbering, the actual mechanics of combining
   work from multiple agents into coherent output.

7. Validation/testing of agent output — How do you know the agent did it right
   before accepting the work?

8. Rollback — Undoing agent work that looked fine but wasn't. Git helps but
   isn't always clean.

9. "Why did it do that?" — Visibility into agent reasoning, not just outputs.

Questions I'm holding for later (or we can address now):

- What are the "gross assumptions" about worktrees/git that don't fit your use case?
- What does "deterministic shell" mean concretely — scripted steps that must run
  in order with validation gates?
- When you say "self improvement," do you mean the system learning from its own
  runs, or something else?
```

**[C]**
```
Should I add these candidate pain points to a separate notes file for later
validation, or do you want to confirm/deny them now before we move to the
positive definition?
```

[10:31]

**[U]** These are all problems and pain points that will come up in the context of using a framework, but these aren't pain points I currently have, nor are they issues that make extant frameworks non-viable for me. It's good thinking though, so let's capture all those thoughts, because they can indicate issues to plan for.

**[U]** And let's put a pin in those questions too, don't forget our notes system needs a way of labeling and finding open questions.

**[U]** I like the question mark sign.

**[C]**
```
Created notes_issues-to-plan-for_2026-01-19-1031.md with the 9 framework-level
concerns to address in design.

Added Open Questions section to process doc using [?] notation:
- ?1: Worktrees/git assumptions
- ?2: Deterministic shell meaning
- ?3: Self improvement definition

Ready to move to positive definition — what this thing IS.
```

**[U]** Can you make your notes on open questions more verbose, e.g. put that in a synthesis block or a thought block? You're writing these notes like someone who will remember this session, but you won't remember anything not written down by the time we revisit any questions.

**[T]** *User is right. I'm writing terse notes as if I have persistent memory, but I don't. Every note needs to be self-contained with full context: what prompted the question, why it matters, what the possible interpretations are, and how the answer affects design decisions. A future session reading "?1: Worktrees/git assumptions" has no idea what that means or why it matters.*

**[C]**
```
Expanded all three open questions in process doc with full context:
- Origin (which transcript, which discussion)
- Context (what the user said that prompted it)
- The actual question spelled out
- Why it matters for the framework design
- Initial guesses at interpretations where relevant

This is a process lesson: all notes must be written for a reader with zero
memory of this conversation.
```

___

*Continued in transcript_2026-01-19-1144.md*

___

**[S]** Process Lesson: Notes for Amnesia

All notes, questions, and synthesis must be written as if the reader has never seen this conversation and never will again. This means:

- Full context on what prompted the note
- Why the item matters for the design
- Initial interpretations or guesses where relevant
- Cross-references to transcript locations for detail

Terse entries like "?1: Worktree assumptions" are useless to a fresh session. Verbose entries that explain the context, the question, and the stakes are essential.
