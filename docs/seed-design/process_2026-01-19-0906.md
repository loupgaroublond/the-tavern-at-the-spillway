# Discovery Process Meta-Document

**Created:** 2026-01-19 09:06
**Updated:** 2026-01-19 11:57


## What This Document Is

This captures the process we're using to develop the system design. It serves as a "pick up where we left off" reference for future sessions.


## The Process

### Two Input Sources

1. **Verbal explanations** — User describes concepts in general strokes, I interview to refine
2. **initial_notes.md** — 47 lines of seed ideas to iterate through line-by-line with explanations and questions


### How We Work

- **I ask questions continuously** — to clarify, fill gaps, validate understanding, explore implications
- **No assumptions** — I ask before assuming, even for small things
- **"What if" scenarios welcome** — User will confirm/deny implications to help triangulate understanding
- **User will ask for my input** — I'm an active participant, may take ownership of areas where my perspective on what works best for AI agents matters
- **High fidelity to user's vision** — This is their concept, not a generic best-practices exercise


### Documentation Approach

- **Location:** `docs/seed-design/`
- Notes broken into files with datetime suffixes (e.g., `_2026-01-19-0906.md`)
- Timestamps added when content is added to existing files
- Files broken up by logical idea boundaries and to avoid overly long files


### Session Continuity

- User directs where to start each fresh session
- I then figure out what to read to rebuild context
- Critical: maintain interview log so nothing lost between sessions


### File Consistency

When information is updated in one file, ensure all related files are updated to match — code, config, documentation, everything. This prevents accruing tech debt where different files say conflicting things. After making a clarification or decision:

1. Identify all files that touch the same concept
2. Update each one to be consistent
3. Code, config, reader doc, vocab files, and notes should all agree


### Tangent Handling

- Leave placemarkers for tangents (see Legend below)
- Maintain a map/legend so we can grep later
- Ask for clarification on: new file vs note-and-punt vs inline exploration


## Interview Transcript Approach

We maintain two parallel artifacts:

1. **Transcript** — Raw interview record with notation (see below)
2. **Synthesis Notes** — Periodically collected/distilled understanding

### Transcript Notation

| Tag | Format | Meaning |
|-----|--------|---------|
| **[U]** | Plain text | User's words (high fidelity to what was said) |
| **[C]** | Code block | My responses |
| **[T]** | *Italics* | My thinking/reasoning (where it adds understanding) |
| **[S]** | after `___` | Synthesis (new whole ideas from recent discussion) |
| **[?]** | inline | Open question (tracked in Open Questions section) |

- **When to synthesize:** When new whole ideas emerge from conversation. Be extra eager when context window is filling up.

- **When to include thinking:** When it fundamentally adds understanding — e.g., showing reasoning that ruled out interpretations A and C in favor of B.

- **Edited transcript:** Raw transcript preserved, but may produce magazine-style edited version for readability.


## Tangent Legend

| Marker | Location | Status | Description |
|--------|----------|--------|-------------|
| (none yet) | | | |


## Open Questions

Questions are marked with **[?]** in transcripts. Each entry includes full context so a fresh session can understand what was asked and why it matters.

---

### ?1 — Worktree/Git Assumptions (open)

**Origin:** transcript_2026-01-19-1026.md, Problem Statement discussion

**Context:** User listed as a pain point that "tools to manage worktrees make gross assumptions about how you use code repos that aren't valid in my use case." This came up twice — once regarding YOLO mode sandboxing ("3rd party tools make assumptions about git and worktrees that are not correct for me") and again regarding agents clobbering each other.

**The question:** What specifically are these assumptions? Examples might include: one worktree per branch, clean working directory expected, specific remote naming conventions, assumptions about .git location, etc. Understanding this is critical because worktree/sandbox design will be a core part of the framework.

**Why it matters:** If we don't understand what assumptions are wrong, we'll repeat them. This shapes the entire isolation/sandbox architecture.

---

### ?2 — Deterministic Shell Meaning (RESOLVED)

**Origin:** transcript_2026-01-19-1026.md, Problem Statement discussion

**Context:** User said "Many of my workflows require a deterministic shell around them to enforce that they are performed correctly, and the Claude Code TUI just offers hooks that can constrain this, without giving enough flexibility."

**The question:** What does "deterministic shell" mean concretely? Initial guess: scripted steps that must run in a specific order, with validation gates between them, where the shell enforces that steps can't be skipped or reordered. But this could mean something else entirely — could be about reproducibility, about capturing exact state, about audit trails, etc.

**Why it matters:** This directly affects workflow engine design. "Hooks that constrain without flexibility" suggests the user needs something more expressive than pre/post hooks — possibly a full workflow DSL or state machine.

---

### ?3 — Self Improvement Definition (RESOLVED — covered by lines 11-12)

**Origin:** transcript_2026-01-19-1026.md, Problem Statement discussion

**Context:** User listed "There's no self improvement of these systems" as a pain point, alongside "There isn't enough telemetry to fully improve it, there's a heavy reliance on brute force."

**The question:** What does "self improvement" mean here? Possibilities:
- The system learning from successful/failed runs to improve prompts
- Automatic workflow refinement based on outcomes
- Agents getting better at task estimation
- The framework itself evolving its own processes
- Human-in-the-loop improvement that's better captured and applied

**Why it matters:** This could be anything from "better logging" to "ML-based prompt optimization" to "evolutionary workflow design." The scope varies enormously depending on what's meant.

---

### ?4 — Agent "Done" Assertion (RESOLVED)

**Origin:** transcript_2026-01-19-1144.md, User Flow discussion

**Context:** User described 5 task modes an agent can be in (execute, delegate, plan, break up, unify) and noted agents can bounce between them. Then said "put a pin in this since we need to talk about what makes an agent 'done' and how we assert this properly."

**The question:** How do we know when a mortal agent's task is complete? Sub-questions:
- Is "done" self-declared by the agent, verified by parent, or confirmed by user?
- What happens if agent says done but work is wrong/incomplete?
- Does "done" mean "code works" or "user accepted" or something else?
- How does this interact with the 5 task modes — is "done" different for each?
- What about partial completion, blocked states, or "done enough for now"?

**Why it matters:** This is the lifecycle boundary for mortal agents. If we get this wrong, we'll have zombie agents or premature termination. Also affects how the dashboard shows task status.

---

### ?5 — Bubbling Up Process (RESOLVED)

**Origin:** transcript_2026-01-19-1144.md, User Flow discussion

**Context:** User said child agents will "bubble up things as needed" when they need user input, and wants to "define it better later."

**The question:** How exactly does bubbling work? Sub-questions:
- Does child ask parent, who decides whether to handle or escalate?
- Or does child message go directly to user with parent CC'd?
- Can messages skip levels (child → grandparent)?
- How does user know which agent originated the question?
- What if parent is busy/blocked — does that delay the bubble?
- Is there prioritization (urgent bubbles vs can-wait bubbles)?

**Why it matters:** This is the core attention-routing mechanism. Get it wrong and user either drowns in notifications or misses critical questions.

---

### ?6 — Perseverance Prompts & Agent Lifecycle (RESOLVED)

**Origin:** transcript_2026-01-19-1144.md, User Flow discussion

**Context:** User described "Ralph Wiggum mode" where system auto-prompts agent to continue. Also mentioned "calling" and "hanging up" send special messages. Wants to discuss "what those are in detail, plus the overall lifecycle of the individual agent and its actions."

**The question:** Multiple interrelated questions:
- What exactly is a perseverance prompt? Just "continue" or more nuanced?
- Does the prompt include context about why the agent stopped?
- What are the lifecycle states of an agent? (spawned → working → waiting → done/hibernating?)
- What triggers state transitions?
- How do "calling" and "hanging up" messages affect agent behavior?
- What's the difference between agent stopping, pausing, and completing?

**Why it matters:** This defines the async agent runtime. The Claude Agent SDK gives us primitives, but we need to design the state machine on top of it.

---

### ?7 — User Consent for New Chats (RESOLVED — principle established, details TBD)

**Origin:** transcript_2026-01-19-1144.md, User Flow discussion

**Context:** User emphasized "agents can make suggestions, but don't just force a new chat on the user, unless it's 100% clear they're OK with it, in that context."

**The question:** What does "OK with it" mean concretely? Sub-questions:
- Is there a user preference setting ("always ask" vs "allow suggestions")?
- Does context matter (e.g., user already said "show me details" = implicit consent)?
- Is there a difference between Jake suggesting and a child agent suggesting?
- What's the UI for "agent wants to open a new chat with you"?
- Can user set per-agent or per-task consent preferences?

**Why it matters:** This is about user control over attention. Too aggressive = annoying. Too passive = user misses important context.

---

### ?8 — Sandboxing for Safety (RESOLVED)

**Origin:** transcript_2026-01-19-1144.md, User Flow discussion

**Context:** User mentioned the system "spares them tedious things, like fine-grained permissions approvals" and said "put a pin in sandboxing to discuss how we do safety." Earlier in problem statement, user noted YOLO mode sandbox doesn't suit their needs and 3rd party tools make wrong assumptions.

**The question:** How do we sandbox agents safely without the pain points? Sub-questions:
- What permissions model? (Per-agent? Per-task? Per-action?)
- How does Jake's "least privileges by quantity" work?
- What isolation mechanism? (Containers? VMs? Worktrees? Something else?)
- How do we avoid the git/worktree assumptions that bother user?
- Is there a trust hierarchy? (Jake trusted to spawn, children less trusted?)
- How does user grant/revoke permissions?
- What's the failure mode when agent tries something disallowed?

**Why it matters:** This is the security model. User wants autonomous agents but also wants safety. These goals are in tension and require careful design.

---

### ?9 — UI Stream Separation (RESOLVED — principle established, details TBD)

**Origin:** transcript_2026-01-19-1144.md, discussion about scrolling problem

**Context:** User pointed out that when I write things down, all the other text scrolls past and they can't read what I said. This is exactly the problem we're solving. User said: "Separating out thinking, vs tool use, vs coding, vs running jobs, vs the discussion in the chat window will be something critical to get right, where the user may want to see the stream of thought, and all the work done so far, but still be able to see the chat itself in a more readable manner."

**The question:** How do we separate different types of content in the UI? Sub-questions:
- What are the distinct stream types? (thinking, tool use, coding, jobs, chat, status updates, errors?)
- Does user see all streams in one view with visual distinction, or separate panes/tabs?
- Can user collapse/expand stream types?
- How does this interact with zoom — does zooming into an agent show all its streams?
- What's the default view vs power-user view?
- How do we handle streams from multiple agents simultaneously?

**Why it matters:** The TUI Claude Code "smooshes everything together" — this is a core pain point. If we don't solve this, we've just built another unreadable interface.

**Additional pain point captured:** TUI Claude Code is hard to read because everything is smooshed together. No separation of streams makes it impossible to follow conversation while work is happening.


## Current State

- **Phase:** PRD COMPLETE

- **Completed:**
  - initial_notes.md walkthrough (all 47 lines)
  - All original open questions resolved (?1-?9, ?19)
  - New questions resolved (?20-?38)
  - PRD written (`prd_2026-01-19.md`)
  - Day 1 questions identified for implementation

- **Next:** Start implementation tomorrow


## initial_notes.md Progress

Tracking line-by-line walkthrough of `initial_notes.md` (47 lines total).

| Line | Status | Summary |
|------|--------|---------|
| 1 | **done** | Preflight check: sandbox, resources, credentials. Fail = no start + parent notified |
| 2 | **done** | 5 sandbox primitives: changeset (overlay FS), platform, isolation, outputs, software. Mix/match per agent. Orthogonal to git. |
| 3 | — | (blank) |
| 4 | **done** | FOUNDATION: Document store = memory backbone. Markdown + structured data. Messaging, work queues, authoritative source. Will expand. |
| 5 | **done** | Project space = root directory (original or overlay). Working context, orthogonal to VCS. |
| 6 | **done** | Workflow engine: state machine, track/enforce steps, manage templates. Composability TBD. Orthogonal to task modes. |
| 7 | **done** | Spec engine (vague): verify output matches spec, detect agent conflicts, check completeness. Details TBD. |
| 8 | **done** | Starter workflow templates: rule of 5 (5 passes), verification layers (lint→structure→arch→perf). User can modify. |
| 9 | **done** | Efficiency: check agents on course, token budgets to cap spending. Telemetry open topic. |
| 10 | **done** | Gang of experts: specialized agents on demand (reviewer, tester, architect). User can customize. |
| 11 | **done** | Meta process: decision layer for selecting workflows/tools. Eye toward improving overall performance. |
| 12 | **done** | Creative meta process creator: measure workflows, improve them. One aspect of ?3 (self-improvement). |
| 13 | **done** | Discovery sharing: prompt agents to notice/share oddities. Can't enforce deterministically, rely on prompt design. |
| 14 | **done** | Frontend = new IDE type: doc store + filesystem, clickable links, conversation mgmt + project exploration. |
| 15 | **done** | Sessions frontend: dashboard, tabs, notifications, zoom. (Covered in User Flow.) |
| 16 | **done** | Context switching: agent card with name + assignment + current status. Quick briefing to jump in. |
| 17 | **done** | Fish or cut bait: triggers = budget exceeded, wrong changeset, spinning, wrong path. Kill + restart. |
| 18 | **done** | Rewind and branch: checkpoint → tweak prompt → fork timeline. Agent + changeset level. |
| 19 | **done** | Active diff: live view of changeset/overlay. (Asked-vs-done is status card.) |
| 20 | **done** | Merge queue: agents queue up, see what's ahead, refine against predictable target. Changesets + repos. |
| 21 | **done** | Telemetry: surface bottlenecks, faults, stale agents. Metrics covered. Detection/alerting TBD. |
| 22 | **done** | Hygiene: cleanup daemons for agents, changesets, overlays, workflows, documents, orphaned resources. |
| 23 | **done** | Surface questions: flexible modes (popup vs periodic summary). Parent chooses approach. User: direct, answer, or own thing. |
| 24 | **done** | Zoom decision: high-level vs low-level agent. User choice + agent suggestion + heuristics. |
| 25 | **done** | Sandbox granularity: covered in line 2 (mix/match primitives, parent picks). |
| 26 | **done** | Question triage: quick vs deep. Notifications convey complexity. |
| 27 | **done** | Session inbox: unread, needs attention, done-needs-closure. User closes loop explicitly. |
| 28 | **done** | Todo + session unified: agent session IS a todo item, todo can spawn session. |
| 29 | **done** | Hooks: mooted — own runtime with SDK, direct control, don't need external hooks. |
| 30 | **done** | (Lines 30-32 = unit) Agent types: daemon (background), mortal (tasks), Jake (top). Covered. |
| 31 | **done** | (see line 30) |
| 32 | **done** | (see line 30) |
| 33 | **done** | On-demand agents, fork more. Covered in scaling. |
| 34 | **done** | Agent tree/list view, can summarize. Dashboard. Covered. |
| 35 | **done** | Messaging: certain messages require certain responses, else agent malfunction. |
| 36 | **done** | Bubbling: open expectation to message up chain to right recipient. Also lateral collaboration. |
| 37 | **done** | Break down tasks: covered in task modes. |
| 38 | **done** | Processes are evolving scripts (covered by lines 11-12: meta process + creative meta process) |
| 39 | **done** | How agents break down evolves (covered by lines 11-12: meta process / self-improvement) |
| 40 | **done** | Talk to mayor or drill into individuals (covered by line 24: zoom decision) |
| 41 | **done** | Mortal agents live for assignment lifecycle (covered by lines 30-32: agent types) |
| 42 | **done** | Daemon agents ensure health (covered by lines 30-32 + line 22: agent types + hygiene) |
| 43 | **done** | Each top agent has naming scheme (covered in vocab_naming-themes) |
| 44 | **done** | Assignment monitored by daemon (covered by daemon role, relates to ?4) |
| 45 | **done** | Break down into units of work (covered by task modes + drones discussion) |
| 46 | **done** | Units of work have steps (covered by line 6: workflow engine) |
| 47 | **done** | Different environments for agents (covered by line 2: sandbox primitives) |

**Status:** COMPLETE
